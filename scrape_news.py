import os
import sys
import time
import pandas as pd
import requests
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from transformers import pipeline

# symbols_config èª­ã¿è¾¼ã¿
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from symbols_config import stock_list

def run_scraper(progress_callback=None, should_stop=lambda: False):
    """
    Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã€BERTã§æ„Ÿæƒ…åˆ†æã€‚
    çµæœã‚’ data/news_sentiment.csv ã«ä¿å­˜ã€‚
    progress_callback(ratio: float) ã§é€²æ—é€šçŸ¥ã€‚
    should_stop(): bool ã§ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒã‚§ãƒƒã‚¯ã€‚
    """
    model_name = "christian-phu/bert-finetuned-japanese-sentiment"
    sentiment_analyzer = pipeline("sentiment-analysis", model=model_name, device=0)

    end_date = datetime.today().date()
    start_date = end_date - timedelta(days=365)
    total_days = (end_date - start_date).days + 1
    total_iterations = len(stock_list) * total_days
    iteration = 0

    all_results = []
    headers = {"User-Agent": "Mozilla/5.0"}

    for stock in stock_list:
        if should_stop():
            return
        company_name = stock["name"]

        for day_offset in range(total_days):
            if should_stop():
                return

            date_obj = start_date + timedelta(days=day_offset)
            ymd = date_obj.strftime("%Y-%m-%d")
            query = f"{company_name} æ ªä¾¡"
            search_url = f"https://news.yahoo.co.jp/search/?p={query}&fd={ymd}&td={ymd}"

            iteration += 1
            if progress_callback:
                progress_callback(iteration / total_iterations)

            try:
                res = requests.get(search_url, headers=headers, timeout=10)
                soup = BeautifulSoup(res.text, "html.parser")
                links = soup.find_all(
                    "a",
                    href=lambda x: x and x.startswith("https://news.yahoo.co.jp/articles/")
                )
                article_urls = list(dict.fromkeys(link["href"] for link in links))
            except Exception:
                continue

            for url in article_urls:
                if should_stop():
                    return

                try:
                    article = requests.get(url, headers=headers, timeout=10)
                    article_soup = BeautifulSoup(article.content, "html.parser")
                    title_tag = article_soup.find("h1")
                    title = title_tag.get_text(strip=True) if title_tag else "No Title"

                    # è¨˜äº‹æœ¬æ–‡æŠ½å‡ºã¨ä¸è¦ãƒ†ã‚­ã‚¹ãƒˆã®é™¤å»
                    paragraphs = article_soup.select("article p, .article-body p, .main-content p, p")
                    # JavaScriptç„¡åŠ¹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å«ã‚€æ®µè½ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    filtered = []
                    for p in paragraphs:
                        text = p.get_text(strip=True)
                        if not text:
                            continue
                        if "JavaScriptãŒç„¡åŠ¹" in text:
                            continue
                        filtered.append(text)
                    body = "\n".join(filtered)
                except Exception:
                    continue

                try:
                    text = body if body.strip() else title
                    result = sentiment_analyzer(text[:512])[0]
                    label = result["label"]
                    score = float(result["score"])
                except Exception:
                    label = "NEUTRAL"
                    score = 0.5

                all_results.append({
                    "date": ymd,
                    "company": company_name,
                    "title": title,
                    "body": body,
                    "label": label,
                    "score": score,
                    "url": url
                })

    # ä¿å­˜
    if all_results:
        df = pd.DataFrame(all_results)
        os.makedirs("data", exist_ok=True)
        df.to_csv("data/news_sentiment.csv", index=False, encoding="utf-8-sig")


# å˜ä½“å®Ÿè¡Œç”¨
if __name__ == "__main__":
    print("ğŸ“„ ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†é–‹å§‹...")
    
    def console_progress(r):
        pct = f"{r*100:5.1f}%"
        print(f"\ré€²æ—: {pct}", end="", flush=True)

    run_scraper(progress_callback=console_progress, should_stop=lambda: False)
    print("\nâœ… ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†å®Œäº†")
